{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c890d6-0c41-4a6f-8b00-921611b9717e",
   "metadata": {},
   "source": [
    "# 5. Classification Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89d8e9-a92c-497c-8797-b86690a61a23",
   "metadata": {},
   "source": [
    "Data prorcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1acdc8-0659-4707-a946-7cc69f6d811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"Combined/FullCleaned.csv\")\n",
    "\n",
    "# Create a new folder to save the CSV files\n",
    "folder_name = 'Indv'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Iterate over each unique stock\n",
    "for stock_name in df['Stock'].unique():\n",
    "    # Filter rows where the Stock column is equal to the current stock\n",
    "    stock_df = df[df['Stock'] == stock_name]\n",
    "    \n",
    "    # Define the filename for the current stock\n",
    "    filename = os.path.join(folder_name, f'{stock_name}.csv')\n",
    "    \n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    stock_df.to_csv(filename, index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee6d75-9dda-4c9e-893e-4adfe528421e",
   "metadata": {},
   "source": [
    "In order to have the 1 day before price we need to divide the combined dataset into individuals companies then add the price from historical prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096364a9-4f67-4fa2-9eb8-7ca56a181b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: 1211.csv\n",
      "Processed and saved: 175.csv\n",
      "Processed and saved: 2333.csv\n",
      "Processed and saved: 5380.csv\n",
      "Processed and saved: 7201.csv\n",
      "Processed and saved: 7202.csv\n",
      "Processed and saved: 7203.csv\n",
      "Processed and saved: 7261.csv\n",
      "Processed and saved: 7267.csv\n",
      "Processed and saved: 7269.csv\n",
      "Processed and saved: 7270.csv\n",
      "Processed and saved: BMW.csv\n",
      "Processed and saved: Ford.csv\n",
      "Processed and saved: GM.csv\n",
      "Processed and saved: LCID.csv\n",
      "Processed and saved: LI.csv\n",
      "Processed and saved: MBG.csv\n",
      "Processed and saved: PII.csv\n",
      "Processed and saved: PSNY.csv\n",
      "Processed and saved: RACE.csv\n",
      "Processed and saved: RIVN.csv\n",
      "Processed and saved: RNO.csv\n",
      "Processed and saved: STLAM.csv\n",
      "Processed and saved: TESLA.csv\n",
      "Processed and saved: VFS.csv\n",
      "Processed and saved: VOLVO.csv\n",
      "Processed and saved: vw.csv\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "indv_dir = 'Indv'\n",
    "cleaned_prices_dir = 'CleanedHisPrices'\n",
    "output_dir = 'ClassDatasets'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get list of files in each directory\n",
    "indv_files = os.listdir(indv_dir)\n",
    "cleaned_prices_files = os.listdir(cleaned_prices_dir)\n",
    "\n",
    "# Helper function to find the most recent previous price\n",
    "def get_previous_price(date, prices_df):\n",
    "    previous_dates = prices_df[prices_df['Date'] < date]\n",
    "    if not previous_dates.empty:\n",
    "        return previous_dates.iloc[-1]['Price']\n",
    "    return prices_df.iloc[-1]['Price']\n",
    "\n",
    "# Process each file\n",
    "for file_name in indv_files:\n",
    "    if file_name in cleaned_prices_files:\n",
    "        # Read the individual dataset\n",
    "        indv_df = pd.read_csv(os.path.join(indv_dir, file_name))\n",
    "\n",
    "        # Read the cleaned prices dataset\n",
    "        cleaned_prices_df = pd.read_csv(os.path.join(cleaned_prices_dir, file_name))\n",
    "\n",
    "        # Convert 'Date' columns to datetime\n",
    "        indv_df['Date'] = pd.to_datetime(indv_df['Date'])\n",
    "        cleaned_prices_df['Date'] = pd.to_datetime(cleaned_prices_df['Date'])\n",
    "\n",
    "        # Sort the cleaned prices dataframe by date\n",
    "        cleaned_prices_df = cleaned_prices_df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "        # Initialize a new column for the previous day prices\n",
    "        indv_df['PreviousDayPrice'] = indv_df['Date'].apply(lambda date: get_previous_price(date, cleaned_prices_df))\n",
    "\n",
    "        # Save the merged dataset to the output directory\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "        indv_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Processed and saved: {file_name}\")\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64bfff-28a3-4d1c-9f7f-5deaaa22ed74",
   "metadata": {},
   "source": [
    "Now after adding most recent previous day price we need to create new column for 1 increasing or 0 decreasing in price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16120849-3d26-4ea7-adda-b890ffeec4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: 1211.csv\n",
      "Processed and saved: 175.csv\n",
      "Processed and saved: 2333.csv\n",
      "Processed and saved: 5380.csv\n",
      "Processed and saved: 7201.csv\n",
      "Processed and saved: 7202.csv\n",
      "Processed and saved: 7203.csv\n",
      "Processed and saved: 7261.csv\n",
      "Processed and saved: 7267.csv\n",
      "Processed and saved: 7269.csv\n",
      "Processed and saved: 7270.csv\n",
      "Processed and saved: BMW.csv\n",
      "Processed and saved: Ford.csv\n",
      "Processed and saved: GM.csv\n",
      "Processed and saved: LCID.csv\n",
      "Processed and saved: LI.csv\n",
      "Processed and saved: MBG.csv\n",
      "Processed and saved: PII.csv\n",
      "Processed and saved: PSNY.csv\n",
      "Processed and saved: RACE.csv\n",
      "Processed and saved: RIVN.csv\n",
      "Processed and saved: RNO.csv\n",
      "Processed and saved: STLAM.csv\n",
      "Processed and saved: TESLA.csv\n",
      "Processed and saved: VFS.csv\n",
      "Processed and saved: VOLVO.csv\n",
      "Processed and saved: vw.csv\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = 'ClassDatasets'\n",
    "output_dir = 'ClassDatasets'\n",
    "\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get list of files in the input directory\n",
    "input_files = os.listdir(input_dir)\n",
    "\n",
    "# Process each file\n",
    "for file_name in input_files:\n",
    "    file_path = os.path.join(input_dir, file_name)\n",
    "    if file_path.endswith('.csv'):\n",
    "        # Read the dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        if 'Price' in df.columns and 'PreviousDayPrice' in df.columns:\n",
    "            # Create the 'Increase' column\n",
    "            df['Increase'] = (df['Price'] > df['PreviousDayPrice']).astype(int)\n",
    "\n",
    "            # Drop the 'Price' column\n",
    "            df.drop(columns=['Price'], inplace=True)\n",
    "\n",
    "            # Rename the 'Increase' column to 'Price'\n",
    "            df.rename(columns={'Increase': 'Price'}, inplace=True)\n",
    "\n",
    "            # Save the processed dataset to the output directory\n",
    "            output_path = os.path.join(output_dir, file_name)\n",
    "            df.to_csv(output_path, index=False)\n",
    "\n",
    "            print(f\"Processed and saved: {file_name}\")\n",
    "        else:\n",
    "            print(f\"Skipping {file_name}: Required columns not found.\")\n",
    "\n",
    "print(\"All files processed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1003b2d-25ae-448f-b2d8-4995dd9443c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to Combined/classification_combined_data.csv.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csv_files(folder_path, output_file):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"Folder path does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "    \n",
    "    # Check if there are CSV files in the folder\n",
    "    if len(csv_files) == 0:\n",
    "        print(\"No CSV files found in the folder.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate through each CSV file and append its data to the list\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list into one\n",
    "    combined_data = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Write the combined data to a new CSV file\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "    print(f\"Combined data saved to {output_file}.\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"ClassDatasets\"\n",
    "output_file = \"Combined/classification_combined_data.csv\"\n",
    "combine_csv_files(folder_path, output_file)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e4bbf-871e-47f2-9e3b-79cef6a5a398",
   "metadata": {},
   "source": [
    "now all combined into 1 csv we need to normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4341a0f3-217c-4235-a326-8819ea419a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization complete. The standardized data has been saved to 'Standardized_Dataset.csv' in the 'Full' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('Combined/classification_combined_data.csv')\n",
    "\n",
    "# Identify the numerical columns excluding \"Price\"\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop('Price')\n",
    "\n",
    "# Standardize all numerical columns excluding \"Price\"\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Specify the directory to save the file\n",
    "output_directory = 'Full'\n",
    "os.makedirs(output_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Save the standardized DataFrame to a new CSV file\n",
    "output_file = os.path.join(output_directory, 'Classification_Standardized_Dataset.csv')\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Standardization complete. The standardized data has been saved to 'Standardized_Dataset.csv' in the '{}' directory.\".format(output_directory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89816f78-e69a-4fa1-a977-eb4ce9a4c6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5754796-5ffd-411f-8e17-9bb10c84ed30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844e2fc-8f5e-4887-9bc2-0a4c535a692a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0ec2924-2b35-494f-9c87-9b35cb6af008",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1c40ce9-cb33-499c-81a3-21853b175d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Accuracy: 0.5124378109452736\n",
      "Precision: 0.5471698113207547\n",
      "Recall: 0.5370370370370371\n",
      "SVM:\n",
      "Accuracy: 0.5970149253731343\n",
      "Precision: 0.6030534351145038\n",
      "Recall: 0.7314814814814815\n",
      "XGBoost:\n",
      "Accuracy: 0.5124378109452736\n",
      "Precision: 0.5462962962962963\n",
      "Recall: 0.5462962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step  \n",
      "Neural Network:\n",
      "Accuracy: 0.6019900497512438\n",
      "Precision: 0.6044776119402985\n",
      "Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"Full/Classification_Standardized_Dataset.csv\")\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Convert 'Date' column to datetime and extract useful features\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "    # Drop original 'Date' and 'Quarter_Date' columns\n",
    "    df.drop(columns=['Date'], inplace=True)\n",
    "    \n",
    "    # Use target encoding for the 'Stock' column\n",
    "    encoder = TargetEncoder()\n",
    "    df['Stock_Encoded'] = encoder.fit_transform(df['Stock'], df['Price'])\n",
    "    \n",
    "    # Drop the original 'Stock' column\n",
    "    df.drop(columns=['Stock'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Assuming 'Price' is the target feature\n",
    "X = data.drop(['Price'], axis=1)\n",
    "y = data['Price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_precision = precision_score(y_test, rf_pred)\n",
    "rf_recall = recall_score(y_test, rf_pred)\n",
    "print(\"Random Forest:\")\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Precision:\", rf_precision)\n",
    "print(\"Recall:\", rf_recall)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_precision = precision_score(y_test, svm_pred)\n",
    "svm_recall = recall_score(y_test, svm_pred)\n",
    "print(\"SVM:\")\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Precision:\", svm_precision)\n",
    "print(\"Recall:\", svm_recall)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_precision = precision_score(y_test, xgb_pred)\n",
    "xgb_recall = recall_score(y_test, xgb_pred)\n",
    "print(\"XGBoost:\")\n",
    "print(\"Accuracy:\", xgb_accuracy)\n",
    "print(\"Precision:\", xgb_precision)\n",
    "print(\"Recall:\", xgb_recall)\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "nn_pred_proba = nn_model.predict(X_test_scaled)\n",
    "nn_pred = np.round(nn_pred_proba).flatten()\n",
    "nn_accuracy = accuracy_score(y_test, nn_pred)\n",
    "nn_precision = precision_score(y_test, nn_pred)\n",
    "nn_recall = recall_score(y_test, nn_pred)\n",
    "print(\"Neural Network:\")\n",
    "print(\"Accuracy:\", nn_accuracy)\n",
    "print(\"Precision:\", nn_precision)\n",
    "print(\"Recall:\", nn_recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600b888-ac37-40ab-909e-9d751c17512e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b26a5-6adc-4e8c-991e-538c7f184c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13dace8-5238-4263-84f3-66d32a334542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16109879-0c0b-44cf-9e31-e6df5c813b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "766403e9-50ad-4a5e-9e47-80363cbad429",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f6e5f4e-305d-45ac-8cbf-e7d544e42ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters:\n",
      "{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best SVM Model:\n",
      "SVC(C=1, gamma=0.1)\n",
      "Best SVM Performance:\n",
      "Accuracy: 0.5920398009950248\n",
      "Precision: 0.6083333333333333\n",
      "Recall: 0.6759259259259259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],  # Kernel coefficient\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel type\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "best_svm_pred = best_svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_svm_accuracy = accuracy_score(y_test, best_svm_pred)\n",
    "best_svm_precision = precision_score(y_test, best_svm_pred)\n",
    "best_svm_recall = recall_score(y_test, best_svm_pred)\n",
    "\n",
    "print(\"Best SVM Parameters:\")\n",
    "print(best_params)\n",
    "print(\"Best SVM Model:\")\n",
    "print(best_svm_model)\n",
    "print(\"Best SVM Performance:\")\n",
    "print(\"Accuracy:\", best_svm_accuracy)\n",
    "print(\"Precision:\", best_svm_precision)\n",
    "print(\"Recall:\", best_svm_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32cf2925-a3a9-4798-b1b3-26f9613d87cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "478 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "62 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.53988354 0.55977484 0.56854814\n",
      " 0.54117236 0.55482143 0.56975155 0.5474146  0.55111025 0.55737578\n",
      " 0.54865683 0.54489907 0.55238354 0.56607143 0.56236025 0.56609472\n",
      " 0.55108696 0.55738354 0.55111025 0.54611801 0.54734472 0.55112578\n",
      " 0.54740683 0.55361025 0.55733696 0.5373913  0.56238354 0.54861801\n",
      " 0.55110248 0.55610248 0.55236801 0.55487578 0.55609472 0.5486413\n",
      " 0.54485248 0.55111025 0.53868012 0.54488354 0.56611025 0.54990683\n",
      " 0.53985248 0.55361801 0.54861801 0.54365683 0.54865683 0.54239907\n",
      " 0.53740683 0.56236801 0.54736801 0.5511413  0.54612578 0.55112578\n",
      " 0.55981366 0.54488354 0.54859472        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.5536646  0.5573913  0.5586413  0.5498913  0.5411413  0.53739907\n",
      " 0.52246894 0.54115683 0.55113354 0.53743789 0.54738354 0.55232143\n",
      " 0.54367236 0.55482919 0.54367236 0.55361025 0.55611801 0.55734472\n",
      " 0.54362578 0.55363354 0.55735248 0.5411413  0.54238354 0.5473913\n",
      " 0.54862578 0.55611801 0.55483696 0.54485248 0.54238354 0.5411413\n",
      " 0.55236025 0.53493012 0.55486801 0.54242236 0.54743012 0.5411646\n",
      " 0.5586646  0.54734472 0.53988354 0.5523913  0.5398913  0.54738354\n",
      " 0.54360248 0.54363354 0.54987578 0.55486801 0.54239907 0.55240683\n",
      " 0.56236025 0.53739907 0.5424146  0.54985248 0.55236025 0.53618012\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.52618012 0.57232919 0.55486025\n",
      " 0.55358696 0.55737578 0.55983696 0.54363354 0.53993789 0.56482143\n",
      " 0.55737578 0.54490683 0.55360248 0.56608696 0.54238354 0.55238354\n",
      " 0.53243012 0.56985248 0.55115683 0.5549146  0.54614907 0.54986025\n",
      " 0.53493789 0.54739907 0.54112578 0.54736025 0.54735248 0.55735248\n",
      " 0.5474146  0.54613354 0.55735248 0.53861801 0.55110248 0.56232919\n",
      " 0.54984472 0.55611801 0.54613354 0.56358696 0.55731366 0.55362578\n",
      " 0.54735248 0.54117236 0.55239907 0.55361025 0.55861025 0.54862578\n",
      " 0.54987578 0.56110248 0.53867236 0.54988354 0.5610559  0.54361801\n",
      " 0.53992236 0.56357919 0.5486413         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55737578 0.54611801 0.55985248 0.54987578 0.55237578 0.54987578\n",
      " 0.55235248 0.54113354 0.55236025 0.55736025 0.55361801 0.55237578\n",
      " 0.55609472 0.55235248 0.55986025 0.55486025 0.55361025 0.54237578\n",
      " 0.56608696 0.54615683 0.54739907 0.53737578 0.54864907 0.54739907\n",
      " 0.55737578 0.55239907 0.54611025 0.55118789 0.55364907 0.54739907\n",
      " 0.55238354 0.54614907 0.54117236 0.54858696 0.53365683 0.54986025\n",
      " 0.5536413  0.55983696 0.55114907 0.54232919 0.54612578 0.55485248\n",
      " 0.54611801 0.55234472 0.55237578 0.55361025 0.55987578 0.54111801\n",
      " 0.53743012 0.54987578 0.5473913  0.54489907 0.55736025 0.55610248]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters:\n",
      "{'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Random Forest Model:\n",
      "RandomForestClassifier(max_depth=20, n_estimators=200)\n",
      "Best Random Forest Performance:\n",
      "Accuracy: 0.5074626865671642\n",
      "Precision: 0.5436893203883495\n",
      "Recall: 0.5185185185185185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object for Random Forest\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_rf_params = rf_grid_search.best_params_\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "best_rf_pred = best_rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf_accuracy = accuracy_score(y_test, best_rf_pred)\n",
    "best_rf_precision = precision_score(y_test, best_rf_pred)\n",
    "best_rf_recall = recall_score(y_test, best_rf_pred)\n",
    "\n",
    "print(\"Best Random Forest Parameters:\")\n",
    "print(best_rf_params)\n",
    "print(\"Best Random Forest Model:\")\n",
    "print(best_rf_model)\n",
    "print(\"Best Random Forest Performance:\")\n",
    "print(\"Accuracy:\", best_rf_accuracy)\n",
    "print(\"Precision:\", best_rf_precision)\n",
    "print(\"Recall:\", best_rf_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d37dbcf8-4e2f-4a26-a911-1c807cd0c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Parameters:\n",
      "OrderedDict([('colsample_bytree', 0.9061979941786817), ('gamma', 0.8593578069828035), ('learning_rate', 0.1570703295827246), ('max_depth', 9), ('min_child_weight', 6), ('n_estimators', 186), ('reg_alpha', 0.7558005328358816), ('reg_lambda', 0.8726303533099421), ('subsample', 0.9559644307534418)])\n",
      "Best XGBoost Model:\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.9061979941786817, device=None,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0.8593578069828035,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1570703295827246,\n",
      "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=186, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n",
      "Best XGBoost Performance:\n",
      "Accuracy: 0.5223880597014925\n",
      "Precision: 0.5535714285714286\n",
      "Recall: 0.5740740740740741\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the parameter search space for XGBoost\n",
    "xgb_param_space = {\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 10),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1.0, 'uniform'),\n",
    "    'colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "    'gamma': (0.0, 5.0),\n",
    "    'reg_alpha': (0.0, 1.0),\n",
    "    'reg_lambda': (0.0, 1.0)\n",
    "}\n",
    "\n",
    "# Create the BayesSearchCV object for XGBoost\n",
    "xgb_bayes_search = BayesSearchCV(\n",
    "    XGBClassifier(),\n",
    "    xgb_param_space,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the bayesian search to the training data\n",
    "xgb_bayes_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_xgb_params = xgb_bayes_search.best_params_\n",
    "best_xgb_model = xgb_bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "best_xgb_pred = best_xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_xgb_accuracy = accuracy_score(y_test, best_xgb_pred)\n",
    "best_xgb_precision = precision_score(y_test, best_xgb_pred)\n",
    "best_xgb_recall = recall_score(y_test, best_xgb_pred)\n",
    "\n",
    "print(\"Best XGBoost Parameters:\")\n",
    "print(best_xgb_params)\n",
    "print(\"Best XGBoost Model:\")\n",
    "print(best_xgb_model)\n",
    "print(\"Best XGBoost Performance:\")\n",
    "print(\"Accuracy:\", best_xgb_accuracy)\n",
    "print(\"Precision:\", best_xgb_precision)\n",
    "print(\"Recall:\", best_xgb_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5147eec9-7b11-47c1-9e11-2c82090ee8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000011FE96437E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000011FF10F56C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Best Neural Network Parameters:\n",
      "{'optimizer': 'adam', 'activation': 'relu', 'batch_size': 32, 'epochs': 50}\n",
      "Best Neural Network Performance:\n",
      "Accuracy: 0.582089552238806\n",
      "Precision: 0.59375\n",
      "Recall: 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Define the function to create the neural network model\n",
    "def create_nn_model(optimizer='adam', activation='relu', batch_size=32, epochs=10):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(64, activation=activation),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid for Neural Network\n",
    "nn_param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [10, 50]\n",
    "}\n",
    "\n",
    "best_nn_accuracy = 0\n",
    "best_nn_params = {}\n",
    "\n",
    "# Perform grid search manually\n",
    "for optimizer in nn_param_grid['optimizer']:\n",
    "    for activation in nn_param_grid['activation']:\n",
    "        for batch_size in nn_param_grid['batch_size']:\n",
    "            for epochs in nn_param_grid['epochs']:\n",
    "                # Create and compile the model\n",
    "                nn_model = create_nn_model(optimizer=optimizer, activation=activation, batch_size=batch_size, epochs=epochs)\n",
    "                print(\"test\")\n",
    "                \n",
    "                # Train the model\n",
    "                nn_model.fit(X_train_scaled, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "                \n",
    "                # Evaluate the model\n",
    "                nn_pred_proba = nn_model.predict(X_test_scaled)\n",
    "                nn_pred = np.round(nn_pred_proba).flatten()\n",
    "                nn_accuracy = accuracy_score(y_test, nn_pred)\n",
    "                \n",
    "                # Update best parameters if accuracy improves\n",
    "                if nn_accuracy > best_nn_accuracy:\n",
    "                    best_nn_accuracy = nn_accuracy\n",
    "                    best_nn_params = {'optimizer': optimizer, 'activation': activation, 'batch_size': batch_size, 'epochs': epochs}\n",
    "\n",
    "# Re-train the best model with the best parameters\n",
    "best_nn_model = create_nn_model(**best_nn_params)\n",
    "best_nn_model.fit(X_train_scaled, y_train, batch_size=best_nn_params['batch_size'], epochs=best_nn_params['epochs'], verbose=0)\n",
    "\n",
    "# Make predictions with the best model\n",
    "best_nn_pred_proba = best_nn_model.predict(X_test_scaled)\n",
    "best_nn_pred = np.round(best_nn_pred_proba).flatten()\n",
    "\n",
    "# Evaluate the best model\n",
    "best_nn_accuracy = accuracy_score(y_test, best_nn_pred)\n",
    "best_nn_precision = precision_score(y_test, best_nn_pred)\n",
    "best_nn_recall = recall_score(y_test, best_nn_pred)\n",
    "\n",
    "print(\"Best Neural Network Parameters:\")\n",
    "print(best_nn_params)\n",
    "print(\"Best Neural Network Performance:\")\n",
    "print(\"Accuracy:\", best_nn_accuracy)\n",
    "print(\"Precision:\", best_nn_precision)\n",
    "print(\"Recall:\", best_nn_recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
